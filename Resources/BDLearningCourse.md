# Учебный курс "Алгоритмы и технологии обработки больших данных"

Марчук А.Г., д.ф.-м.н., профессор

## Введение

Цели курса – дать студентам знания, умения и навыки, необходимые для понимания и работы с большими базами данных. Для продвинутых студентов этот курс позволит начать работу по осваиванию программирования специализированных баз данных.

В учебном пособии активно используется авторская разработка: библиотека PolarDB, предназначенная для создания специализированных и универсальных баз данных и СУБД. Практическая часть курса выполнена в среде программирования .NET Core на языке программирования C#. Используемые компоненты можно будет брать из разных мест, об этом будет написано. Каждому студенту полезно будет создать единое решение Solution для выполнения заданий и экспериментов, желательно пометить его фамилией. Также рекомендуется создать некоторую единообразную струкутру в данном проекте. Рекомендуется создать выделенную директорию Databases для помещения туда тестовых баз данных и других временных файлов. Иначе у Вас уже не нужные файлы, иногда значительных размеров, будут разбросаны по проектам. Также рекомендуется сгруппировать будущие проекты в директории src. Все эти предварительные действия лучше сразу сделать в VisualStudio. Кроме того, предположительно Вам пригодятся мои решения по проектам, да и текст учебного пособия (этот документ), имеющийся в разделе Resources также понадобится. Поэтому, "рядом" со своим решением, поместите мое решение. В командном интерпретаторе это делается командой:
```
git clone https://github.com/agmarchuk/BDLearningCourse.git
```  

## Task01_HelloBigData: Файл, как основа работы с хранимыми данными. Моя первая база данных на C#

Традиционным началом для изучения программных технологий, является написание простейшего приложения-приветствия. Мы пойдем дальше, попробуем «прикоснуться» к проблематике больших данных через написание достаточно простой программы.  Одновременно, будем осваивать использование работу с файлами и стримами (streams). В конечном счете, именно файл почти всегда является носителем сохраняемых данных вне зависимости от их объема. 

Все задание можно выполнить в VisualStudio. Однако, для того, чтобы лучше понимать процессы формирования и исполнения кода, рекомендуется выполнить это задание в командном интерфейсе автономной среды .NET Core. Интерфейс достаточно дружественен и проблем с выполнением простых программ, как правило, не возникает. 

Если Вы искушенный в .NET Core программист, то прошу прощения за известные технологические детали, если Вы новичок, то прошу прощения за упущенные в объяснении детали, которые Вам придется осваивать самостоятельно. 

Текущий вариант загрузки SDK можно найти напр. в https://docs.microsoft.com/en-us/dotnet/core/get-started

Командный интерфейс, автономной среды .NET Core доступен сразу после установки этой системы. Собственно команды набираются в любом интерпретаторе командных строк, напр. встроенном в Windows. Стандартными средствами cd, надо «добраться» до директории, в которой Вы хотите работать, создать там директорию, напр. mkdir HelloWorld, для выполнения задания, перейти вовнутрь. Дальше начинаются содержательные действия. Главная программа, которую надо запускать в командной строке – dotnet.

Запускаем:
```
dotnet new console
```
В директории появится файл .csproj проекта и тестовая программа Program.cs.  Далее, нужно обновить библиотечные файлы
```
dotnet restore
```
После этого можно скомпилировать программу и исполнить ее
```
dotnet build 
dotnet run
```
Исполненная программа выдаст в консоль Hello World!
Дальше можно модифицировать программу Program.cs и добавлять новые модули. Для запуска, надо повторять Dotnet build Dotnet run, или можно только dotnet run. В этом случае, сначала программа будет компилироваться, а потом исполняться. Компиляция может выявить ошибку, об этом будет сказано в диагностическом сообщении и детали сообщения помогут Вам выявить и исправить ошибку. Комментарии будут на английском и обычно их не слишком сложно понять. Учите английский – это часть профессии программиста!

Такая программа слишком проста и мы сразу «двинемся» дальше – к большим данным. Можно производить редактирование программы Program.cs с помощью любого текстового редактора, но уже будет полезным осваивать специализированные средства. К таким относятся Visual Studio и Visual Code. Новичкам я рекомендую студию. Упомянутый проектный файл .csproj выполнен в формате Visual Studio и если студия у Вас загружена, можно из консоли запустить его, напр. start HelloWorld.csproj
Или можно запустить студию обычным образом и потом связать ее с созданными директорией и проектным файлом. Пробуйте.

Будем модифицировать программу в сторону работы с файлом. Для этого, полезно указать в ее начале программы то, что нам нужно соответствующее пространства имен. Начало программы теперь будет выглядеть так:
```
using System;
using System.IO;
```
Обратите внимание, что студия довольно хорошо подсказывает при наборе текста. Поэтому, если не торопиться, можно увидеть массу полезной информации. Это не заменяет документации, но очень помогает тому, кто такую документацию читал, но подзабыл детали. 

Первая часть задания будет заключаться в том, чтобы создать файл и наполнить его данными. Файл создается одной строчкой типа:
```
    FileStream fs = File.Open("data.bin", FileMode.OpenOrCreate);   
```
Добавив строчку в программу и исполнив ее, мы увидим, что в рабочей директории появился файл нулевой длины с указанным именем. При повторном исполнении, ничего не меняется, строчка работает также как способ подсоединения к файлу. Что мы получили в программе? Мы получили поток байтов, доступный для записи и чтения. Будем осваивать бинарные запись и чтение (бинарный – это не текстовый, детали разницы сейчас не существенны). Бинарные запись и чтения выполняются через Write() и Read(), которые не очень удобны для наших текущих целей, поэтому сразу освоим специальные средства для этого. Это BinaryWriter и BinaryReader, создаваемые на основе стрима, в данном случае , файлового стрима. Сначала нам нужен писатель, добавляем строчку:
```
    BinaryWriter bw = new BinaryWriter(fs);  
```
Через райтер можно подряд писать значения разных типов C# просто добавляя через bw.Write(val). При этом, значение будет записано в поток байтов с занятием нужного количества следующих байтов. Например,  мы можем записать длинное целое bw.Write((long)333) и при этом, будет занято 8 следующих байтов. Для прозрачности, запишем МНОГО длинных целых, напр. 100 млн. Причем будем писать значения по порядку: 0, 1, 2, …
```
    long nelements = 100_000_000;
    for (long ii = 0L; ii < nelements; ii++) bw.Write(ii);
```
Программа выполнилась довольно быстро (у меня около 2.2 сек.). Посмотрим на результирующий файл. Он оказался несколько меньше, чем ожидалось: у меня 799 997 952 байта. Дело в том, что FileStream имеет буфер и буфер надо сбрасывать методом Flush(). Добавим строчку:
```
    fs.Flush();
```
Теперь все в порядке. Мы удивительно быстро создали примитивную базу данных, расположенную в файле. 

Вторая часть задания будет заключаться в имитации работы с базой данных. Работа будет заключаться в том, что по заданному случайным образом индексу, мы будем читать то длинное целое, которое ранее туда записали. Эдакий Random Access. Чтобы осуществлять обратное к записи действие, а именно чтение, надо создать бинарного читателя аналогично писателю:
```
    BinaryReader br = new BinaryReader(fs);
```
И с его помощью можно будет читать значения разных типов, напр. длинные целые. Но с какого места? С того, на котором стоит «головка» чтения, она же головка записи. Поэтому сразу прочесть ничего не получится (проверьте!), поскольку головка записи стоит в самом конце файла. Все очень просто: fs.Position = … установит эту головку туда, куда нам нужно. Причем позиция задается как двойное целое. На сначала, надо 
проверить на единичном чтении, что в файле правильные значения. Мы помним, что записывалось длинное целое, соответствующее номеру записи, проверочный фрагмент может быть:
```
    fs.Position = nelements * 2 / 3;
    long v = br.ReadInt64();
    Console.WriteLine($"v = {v}"); 
```
Шестерки покажут, что все получилось правильно. Единичная выборка выполняется слишком быстро, для того, чтобы засечь время выполнения, поэтому будем "прыгать" по файлу случайным образом и читать. И сравнивать с тем, что должно получиться. В итоге, напишем фрагмент типа:
```
    Random rnd = new Random();
    long nreads = 100_000;
    for (long ii = 0; ii < nreads; ii++)
    {
        long ind = rnd.Next((int)nelements);
        fs.Position = ind * 8;
        long val = br.ReadInt64();
        if (val != ind) throw new Exception($"Err: ind={ind} val={val}");
    }    
```
В котором мы не только выполняем требуемое действие, но и проверяем то, что ранее было записано нужное число. Обращаю Ваше внимание на то, что читаем мы не 100 миллионов раз, а только 100 тысяч. Это потому что чтение или запись «подряд» гораздо быстрее чтения или записи по случайному индексу. У меня такой цикл чтений выполняется около 0.4 сек. Кстати, пора начать измерять временные интервалы. Предпочитаю делать это с помощью класса System.Diagnostics.Stopwatch – специализированного секундомера. Созданный объект запускается через метод Start() или Restart(), измерение интервала времени останавливается методом Stop(), после этого, в объекте можно взять измеренный интервал через напр. sw.ElapsedMilliseconds.

Вспоминаем, что мы создаем примитивную базу данных, а база данных должна хранить накопленные данные и после завершения программы, отключаем (напр. комментированием) фрагмент записи данных, оставляем только открытие файла и цикл чтения и убеждаемся, что программа работает. Если говорить в терминах доступа к базе данных, то получаются очень хорошие результаты. Однако, не спешите! Перезагрузите компьютер, запустите программу снова в режиме только чтения. Что получилось? Скорее всего, Вы не дождались окончания работы программы. Наверное, Вы решили, что программа остановилась. Но нет, она работает. Приблизительно в тысячу раз медленнее, чем ранее! В чем дело? Дело в том, что доступ к файлу в современных операционных решениях осуществляется с использованием системного кеша. При записи в файл, его странички «осели» в кеше и выборка происходила довольно быстро. Перезагрузка машины привела к очищению системного кеша, поэтому доступ к содержимому диска стал выполняться в темпе работы диска. А это, для HDD – приблизительно 100 (Random Access) доступов в секунду. Замерьте время последнего теста для разумного количества испытаний и убедитесь, что приблизительно эти скорости и получаются. Этот эффект называется «холодая» база данных, а способ ее активизации – «разогревом». Как правило, при достаточно длительной работы базы данных, она переходит из холодного в разогретое состояние естественным путем. Но не всегда этот способ является быстрым. Зная природу охлаждения и разогрева данных, легко предложить свой способ эффективного разогрева нашей базы данных. Достаточно напр. предварительно прочитать данные (и ничего с ними не делать), для того, чтобы все восстановилось. Для эксперимента, можно скопировать напр. средствами файл-менеджера файл базы данных (после этого, копию можно уничтожить) – это даст желаемый разогрев. Или можно выполнить фрагмент программы:
```
    byte[] buffer = new byte[1000000];
    int nblocks = (int)(nelements * 8 / buffer.Length);
    for (int i = 0; i < nblocks; i++) fs.Read(buffer, 0, buffer.Length);
```
Эксперименты с разогревом показывают, что несмотря на то, что разогрев (почему-то) работает дольше, чем просто запись (у меня 9 сек.), после этого, проблем с производительностью не наблюдается. 

Однако, разогрев будет «работать» только до определенного размера, на моем компьютере – до приблизительно 1 млрд. элементов (по 8 байтов). Это потому, что оперативная память, используемая для системного кеширования работы с файлами, у меня как раз 8 Гб. На самом деле, переход от кеширования к некешированию происходит довольно резко. На моем компьютере, 10 тыс. доступов к анализируемой базе данных в 700 млн. элементов выполняется за 50-80 мс., а к базе данных в 1 млрд. элементов – 31 сек.! 

Мы вышли на принципиальные моменты. Можно попробовать сделать следующие выводы:
1)	Для базы данных существенным фактором, влияющим на скорость работы с данными, является кеширование. Имеется системное кеширование, которое позволяет делать системы со скоростью около 100 тыс. доступов в сек. 
2)	При превышении объема активных данных размера ОЗУ, схемы кеширования становятся неэффективными и скорость доступа будет определяться скоростью доступа к внешнему носителю. Для HDD это около 100 доступов в сек. 
3)	Кроме оптимизации алгоритмов работы с данными, очень важным является эффективная реализация разогрева данных, т.е. перевода активной части данных в кеш.   
  
P.S. Не забудьте уничтожить файл с данными, он все-же под гигабайт!

## Task02_SQLite: Осваиваем SQL в варианте SQLite

В настоящее время, подавляющее число баз данных делаются как связанные реляционные таблицы и погружаются в реляционные СУБД. Эти СУБД концентрируют в себе труд множества разработчиков, иногда - несколько поколений разработчиков, являются эффективными промышленными решениями. Некоторая проблема стандартных СУБД заключается в том, что они универсальны, расчитаны на приенение в очень разных условиях, а поэтому 
не очень позволяют получать предельное по качеству решение для специальных задач или условий применения. 
В частности, эффективность универсальных решений может быстро деградировать при увеличении размеров или сложности данных. 

Мы познакомимся с СУБД SQLite, про которую авторы говорят, что на ней реализована большая (с ударением на первый слог) часть баз данных в мире. Это похоже на правду, поскольку другие СУБД, расчитаны на серверное использование, а эта - очень компактная и может эффективно работать в составе практически любого решения как подключаемая библиотека. В частности, SQLite используют в приложениях для мобильных устройств (вот и most part!). 

SQLite является свободно распространяемым программным обеспечением (ПО), более того, с открытым кодом. Последнее нам не понадобится. SQLite поддерживается через сайт sqlite.org, где можно найти собранные варианты для разных платформ и документацию для грамотного пользования. Для использования совместно с .NET имеется адаптированное для .NET решение, его можно скачать и подключать к проекту как другие библиотечные слои. Но более удобным, является использование NuGet - технологии динамического подключения к специально оформленным модулям. NuGet дает возможность не только разово скачать желаемую систему, но и будет заботиться об обновлениях этой системы. 

Заведем проект. Кстати о названии проектов. Категорически не приветствуюется называть проекты типа ConsoleProj1 и помещать его куда попало. Это же касается и других идентификаторов. Ленность мысли, "двигающая" вами, ни к чему хорошему не приведет. Важно называть объекты мнемонично и в системе соглашений имеющейся в проекте, в системе программирования. Например, в проектах на C# принято названия пространств имен, классов, методов (и проектов) начинать с большой буквы, а названия переменных - с маленькой. Но существеннее именно мнемоничность. Экономьте свои и чужие усилия по написанию и чтению кода. Ответы на вопросы "как же я назвал?" или "что бы это значило?", можно существенно облегчить через общие правила называния и закладываемую мнемонику. Кроме того, небольшое умственное усилие по выдумыванию "хорошего" названия, будут постоянным тренингом ваших мозгов. 

Вернемся проекту. Я не умею запускать SQLite совместно с .NET Core, поэтому рекомендую использовать более старый, но стандартный вариант проекта, поищите "Консольное приложение (.NET Framework)". Создайте его, проверьте, что оно работает. Сразу загрузим библиотеку SQLite. Для этого, (через правую кнопку) выходим на "Управление проектами NuGet", устанавливаем резим "Обзор" и ищем SQLite, находим System.Data.SQLite - это то, что надо, там около полутора миллионов скачиваний, устанавливаем. Куда-то это скачивается, нам не важен ни состав пакета ни место расположения, все это не сложно выяснить для тех,кому это интересно. Уже можно пробовать. 

Добавим используемые пространства имен:
```
using System.Data.SQLite;
using System.Data.Common;
```
База данных SQLite создается и развивается в файле, если файла нет, нужно его создать.
```
    string path = "../../../../";
    string filename = path + "databases/test.db3";
    if (!System.IO.File.Exists(filename))
    {
        SQLiteConnection.CreateFile(filename);
    }
```
Подумайте над тем, в каком месте создается файл базы данных, создайте его. Проверьте работу, убедитесь, что файл создается. Ключевую роль в работе с базой данных, играет connection (коннектор, адаптер, соединитель). Заметим, что то, что было написано не порождает объекта, через который можно будет работать база данных. Создадим такой объект connection:
```
    DbProviderFactory factory = new SQLiteFactory();
    DbConnection connection = factory.CreateConnection();
    connection.ConnectionString = "Data Source=" + filename;
```
Коннектор сам подключится с базе данных через указание строки подключения. Работа с базой данных, как уже было сказано, выполняется через connection, причем для отдельного блока работы надо connection открыть, а по завершении - закрыть. Между этими "скобками" задаются действия. 

Будем создавать базу данных, состоящую из одной таблицы persons. Сначала почистим базу данных конструкцией DROP TABLE persons;
```
    connection.Open();
    DbCommand comm = connection.CreateCommand();
    comm.CommandText = @"DROP TABLE persons;";
    try { comm.ExecuteNonQuery(); }
    catch (Exception ex) { Console.WriteLine($"Warning in DROP section {ex.Message}"); }
    connection.Close();
```
 У персоны (у отдельной записи) есть ключевой целочисленный идентификатор, строковое имя, целочиленный возраст. 
```
    connection.Open();
    comm.CommandText =
    @"CREATE TABLE persons (id INTEGER PRIMARY KEY ASC, name TEXT, age INTEGER);";
    try { comm.ExecuteNonQuery(); }
    catch (Exception ex) { Console.WriteLine($"Warning in CREATE TABLE section {ex.Message}"); }
    connection.Close();
```
При первом запуске, команда DROP TABLE "ругнется", это нормально. Но потом уже будет что уничтожать, поскольку таблица персон создана. Теперь надо загрузить базу данных тестовыми данными. Изолируя каждую команду скобками открытия и закрытия, получаем что-то вроде:

```
    int npersons = 100;
    for (int i = 0; i < npersons; i++)
    {
        connection.Open();
        comm = connection.CreateCommand();
        comm.CommandText = "INSERT INTO persons VALUES (" + i + ",'" + i + "', 21);";
        Console.Write($"{comm.CommandText}");
        comm.ExecuteNonQuery();
        connection.Close();
    }
```

Следующий этап - выполнение запросов на получение данных. Б



```
    // Получение записи по ключу
    Random rnd = new Random();
    sw.Restart();
    for (long i = 0; i < 1000; i += 1)
    {
        connection.Open();
        var com = connection.CreateCommand();
        //int key = (int)(npersons * 2 / 3);
        int key = rnd.Next((int)npersons);
        com.CommandText = "SELECT * FROM persons WHERE id=" + key + ";";
        object[] res = null;
        var reader = com.ExecuteReader();
        int cnt = 0;
        while (reader.Read())
        {
            int ncols = reader.FieldCount;
            res = new object[ncols];
            for (int j = 0; j < ncols; j++) res[j] = reader.GetValue(j);
            cnt += 1;
        }
        if (cnt == 0) { Console.WriteLine("no solutions. key = {key}"); }
        else if (cnt > 1) { Console.WriteLine("multiple solutions. key = {key} cnt = {cnt}"); }
        //Console.WriteLine($"{key} => {res[0]} {res[1]} {res[2]}");

        reader.Close();
        connection.Close();
    }
    sw.Stop();
    Console.WriteLine($"duration {sw.ElapsedMilliseconds}");
```
По сравнению с записью и чтением в файл из файла, временные характеристики формирования базы данных и выполнения выборок, выглядят слабыми, а по записи данных (100 записей за 19 секунд!) - удручающими. В чем дело? В транзакциях. Про возможности использовать более эффективную схему вычислений, в которой транзакции не мешают, а помогают вычислениям, мы поговорим позже, а пока подумаем о том, какими другими способами можно ускорить хотя бы ввод данных. Главный способ - группирование запросов. Это общее решение - чем больше удается объединить элементов запросоа или запросов в одном SQL-запросе, тем лучше. В некоторых случаях это не так, но сейчас это не существенно. На сначала попробуем сгруппировать команды ввода под одним открытым connection'ом. Легко можно перенести скобки открытия и закрытия соединения за цикл и попробовать выполнить ввод. Попробовали - никакого эффекта. Не суть важно почему так, продолжим исследование. Существенно больших результатов можно достигнуть группируя единичные операторы ввода INSERT в групповой оператор INSERT INTO persons VALUES (...),(...),...;.
Вряд ли можно "загнать" один оператор весь поток ввода, но вводить группами по сколько-то сотен или тысяч можно. Эффект - очень заметный и практически линейный. И все же это не самый лучший способ оптимизации, тем более, что для других операторов, групповой конструкции может не оказаться или она может быть неэффективной. Вернемся к транзакциям.   

Транзакция - это последовательность действий, защищенных от взаимодейстия с другими действиями. Другое свойство транзакции - если транзакция не могла закончиться успешно, СУБД автоматически восстанавливает состояние, которое было до начала попытки выполнения транзакци. Самый простой способ защиты действий - последовательное выполнение. Но в "стандартных" для сервера данных условиях, когда транзакции "сыпятся" на сервер асинхронно, приходится защищать эти транзакции специальным образом. Почему нельзя просто упорядочить транзакции по времени и не выполнять их последовательно? Можно, но это может оказаться неэффективно. Действительно, самая массовая в практике транзакция отдельная выборка по SELECT. В простом случае, она выполняется быстро, но если асинхронно в потоке есть сложные, медленные транзакции, хочется не слишком задерживать простые быстрые дожидаясь окончания медленных.

Сейчас нам транзакции нужны для вполне определенного дела - группирования действий в единое и мы вполне можем мыслить последовательным потоком команд доступа к данным. Но как это реализовать? В SQL базах данных имеется методика реализации группы действий в рамках одной (большой) транзакции. Суть методики заключается в том что кроме объекта команды (класса DbCommand) явно создается объект класса DbTransaction, транзакция открывается (transaction.Open()) и "подклеивается" к команде. Далее, можно многократно задавать текст команды и исполнять ее. В конце, транзакцию надо закрыть выполнив transaction.Commit().  

Попробуем. У нас довольно плохо с вводом данных, даже в случае группирования команд INSERT. Начнем с этого места. В принципе, получается прозрачный код обработки, а именно:
```
    connection.Open();
    DbCommand runcommand = connection.CreateCommand();
    runcommand.CommandType = CommandType.Text;
    DbTransaction transaction = connection.BeginTransaction();
    runcommand.Transaction = transaction;
    for (int i = 0; i < nelements; i += 1)
    {
        runcommand.CommandText = "INSERT INTO persons VALUES (" + i + "' + i + "', 21);";
        runcommand.ExecuteNonQuery();
    }
    Console.WriteLine();
    loadtransaction.Commit();
    connection.Close();
```
Внимательно изучите код, прежде, чем вносить его в программу. Все просто, но нетривиально. После модификации фрагмента ввода, Эта часть программы сильно ускоряется. Теперь 100 тыс. записей загружаются за 0.7 сек., а 1 млн. - за 6 сек. То же самое можно сделать и во фрагменте выборки данных по случайно задаваемым ключам. Ускорение будет не столь драмматическим, но также вполне заметным. У меня, базовый вариант дает 300 мс. на 1000 запросов, если вынести открытие/закрытие коннектора за цикл, получается 100 мс., если использовать транзакцию - 35 мс. 

Вообще, измерение времени выполнения тех или иных действий - дело непростое. Сказываются и накладные расходы на организацию действия и попадание или непопадание данных в кеш. И что-то еще, включая способ трансляции и способ запуска. Тем не менее, измерение и сопоставление - это способ что-то утверждать о создаваемой или изучаемой программы. Но это измерение должно производиться правильно. Что это означает? В разных случаях разное, но главное, что методику тестирования и измерения надо обосновывать, обосновывать надо и применение методики к конкретным обстоятельствам. Современные системы полны инженерных особенностей, которые не лежат на поверхности и требуются усилия для того, чтобы провести грамотное тестирование и по измеренным результатам делать обоснованные выводы. 

### Тест "Фототека"
Теперь изучим и проанализируем тест "Фототека", предназначенный для тестирования и сопоставления разных решений в области работы с базами данных. Тест сделан намеренно максимально простым и легко воспроизводим в рамках практически любого решения. Тест сделан в парадигме связанных таблиц и приспособлен для максимально эффективной реализации на обычных SQL-платформах. 

Данные теста

База данных состоит из трех таблиц: персон, фотографий, отражений. Параметр тестового набора - количество записей в таблице персон или просто количество персон npersons. Количество фотографий - npersons * 2. Количество отражений npersons * 6. Все таблицы имеют идентификационное поле id, являющееся первичным ключом для таблицы, все ключи в одной таблице различны и в нашем построении - целочисленны. Таблица персон persons содержит три поля: целочисленный идентификатор id, строковое значение имени name, числовое (возможно целое или длинное целое или вещественное, это зависит от целей тестирования) значение возраста age. Таблица фотографий photos состоит из полей id и name,таблица отражений reflections содержит поля id, reflected, indoc. Причем два последних поля представляют внешние ссылки (external keys) на соответственно таблицу персон (отражаемое) и таблицу фотографий (отражение в документе). 

Таблица персон обычно заполняется следующим образом: в цикле по целочисленному i от 0 до npersons - 1, запись состоит из тройки (npersons-i, (npersons-i).ToString(), random.Next(150)). Практически также заполняется таблица фотографий, только их количество другое и третье поле отсутствует. Отражения заполняется по первому полю аналогично, а по второму и третьему, это псевдослучайная величина в диапазоне 0 - (npersons-1) и 0 - (nphotos-1) соответственно. В некоторых случаях, в таблице отражений колонка первичного ключа может отсутствовать - она в испытаниях, как правило, не используется. 

Тут надо обратить внимание на то, что диапазон идентификаторов не совпадает с интервалом 0 - nelements-1. Немного, всего на пару значений. Это сделано специально, для того, чтобы иметь варианты отсутствия записи с заданным (в стандартном диапазоне) ключа или отсутствия объекта ссылки для таблицы отражения. Такая особенность не является обязательной, а лишь где-то сможет помочь выявить не предусмотренные варианты обработки. Если система не позволяет иметь "висящие" ссылки, то лучше генерировать строгие данные, напр. для ключа формули может быть i или nelements - i - 1. Второй вариант предпочтительнее, поскольку ключи вводятся в обратном сортировочном порядке и создание внутреннего индекса скорее всего будет не столь тривиальной задачей, а поэтому выстраивание индекса по первичному ключу будет скорее всего заметно в измерениях.

Имена для персон и фотографий формируются следующим образом: это либо строковое значение идентификатора id или строковый код, а за ним идентификатор. Такая схема достаточно удобна, в частности, для проверки выдаваемых результатов. Кроме того, лексикографическое упорядочивание тестов чисел не совпадает с числовым упорядочиванием.

Тестовые процедуры

Тестирование и измерение проводятся по небольшому, но принципиальному спектру вопросов. Это - время загрузки, отдельно или вместе - время вычисления индексов. Это - время выборки записей по задаваемому случайным образом ключу, это - время выборки записей по задаваемому имени или части имени. Это - время вычисления "информационного портрета" персоны. Под этим вычислением понимается получение всех фотографий, на которых отражена персона с случайно задаваемым первичным ключем. Времена измеряются в миллисекундах или в секундах, для больших значений. Для "быстрых" действий по выборкам, измерения производится на подходящем количестве испытаний и приводятся к миллисекундам на 1 тыс. испытаний. 

Генератор псевдослучайных чисел должен работать без фиксированного "посева", иначе иногда возникают ситуации, что при повторных запусках, вычисления "пробегают" по относительно небольшому количеству сохраненных в кеше значений. 

Отдельная ситуация с поиском по имени. В принципе, есть два основных вариантов поиска, поиск по полному совпадению с образцом и поиск по частичному совпадению с образцом. Проблема в том, что часто это принципиально разные поиски. Поиск по полному совпадению может базироваться на эффективных решениях типа хеш-таблиц. Поиск по близости - это очень разные варианты. Причем большая часть вариантов трудны для индексации. Самый простой и не очень трудный - поиск по совпадению образца с началом строкового поля. Тогда нужно построить индекс, сортирующий лексикографически элементы таблицы и пользоваться дихотомией - рекурсивным бинарным прохождением индекса. Но дургие варианты похожести образца со строкой, напр. Contains(), уже плохо индексируются и, как правило, для них требуется сканирование данных. Сканированием назовем процесс перебора записей с последующий их фильтрацией. Часто используется еще похожесть через вычисление регуляного выражения. Способ универсальный, но кроме сканирования, трудно что-то предложить. 

Все измеряемые характеристики в том или ином варианте зависят от основного параметра - числа персон. Рекомендуемые значения параметра: 40 тыс., 400 тыс., 4 млн., 40 млн. Последнее количество - уже большие данные. То есть данные, не помещающиеся в оперативную память и которые поэтому очень сложно обрабатывать. Такое предпочтение (4) в скважности тестирования объясняется тем, что 40 тыс. в терминах триплетных высказываний - 1 млн. Соответственно 40 млн. - 1 млрд. триплетов, а это до сих пор является довольно большим количесовом для обработки. Есть еще дополнительные характеристики, которые имеет смысл иногда поизучать, но пока по ним не ведется общей статистики. Это - размер файла или файлов базы данных, предельный размер данных, которые системой обрабатываются, Нагрузка на ОЗУ и др.

После того, как мы определили тест "Фототека", следует попрактиковаться. В данном задании по изучению SQLite, требуется сформировать программу. В программе будет создаваться три таблицы, таблицы будут заполняться данными отталкиваясь от размера npersons, далее должны быть в цикле произведены вычисления записей по ключам, записей по именам, вычисление портретов. В принципе, вы все уже знаете, поэтому объяснять решение шаг за шагом нет необходимости, попробуйте выполнять задание самостоятельно. Причем самая желательная форма - не копируя участков предыдущего решения и не торопясь подгляывать в мое решение.  

## Task03_Polar.DB: Начинаем осваивать Поляр
В практической части данного курса будет использоваться библиотека работы с данными PolarDB или просто Поляр. Эта библиотека построена на небольшом количестве идей, принципов и решений, позволяет компактно и эффективно реализовывать специализированные построения.

Поляр построен на некотором представлении о типизации [], которое созвучно ряду других. Суть этого построения в том, что все рассматриваемые структурные объекты обладают внешним относительно себя типом и рассмотрение этих объектов корректно только в контексте его типа. В принципе, это как во многих других структурных построениях, напр. в C# можно написать оператор:
``
int[] arr = {1, 2, 3, 99};
``
При этом, int[] - определение типа структурного объекта, а справа имеется собственно структурный объект, точнее - его текстовое изображение, потому что в оперативной памяти, объект представляется по-другому. Мы можем изменить строчку на: 
``
double[] arr = {1, 2, 3, 99};
``
И это уже будет совсем другая структура!

Альтернативой такому подходу является структуризация в так называемых "безтиповых" языках программирования, напр. в JavaScript. При таком подходе, тип присутствует, но он "встроен" в структурное значение.

Для лучшего понимания следующего материала, полезно будет познакомиться с элементами теории поляровской структуризации, см. Приложение А настоящего пособия. Можно не слишком досконально вчитываться в приложение, поскольку сами примеры будут вести понимание от простого к сложному. 

Поработаем с тестовыми структурными значениями. В основном мы будем работать в среде .NET Core. Среда хороша в частности тем, что позволяет создавать эффективные решения, работающие под Windows, Linux и iOS. Библиотека Поляра для целей учебного курса доступна через Nuget пакеты набор DLL'ек, размещенных в директориях Resources\DLLs\PolarDBdlls\[Debug | Release]. На стадии обучения и осваивания, рекомендуется использовать группу Debug и в этом же режиме компилировать и исполнять задания. При испытаниях кода на производительность, можно вручную поменять библиотеки и режим на Release.    

Начинаем формировние тестового кода. Заведем консольный проект .NET Core. Добавим ссылки на поляровские DLL'ки. Чтобы не требовалось указывать длинные имена, добавим используемые пространства имен:
```
using Polar.DB;
using Polar.Cells;
```
Еще важный момент - надо указать куда мы будем помещать файлы базы данных. В .NET Core, также как и ранее в .NET, рабочая директория по умолчанию совпадает с директорией где находится запускаемый EXE или DLL (через dotnet Prog.dll). Но если приложение запускается в "стандартном" режиме через Visual Studio или dotnet run, т.е. с использованием проектного файла, то именно место проектного файла является рабочей директорией. С учетом этого, определим path к директории будущей базы данных:
```
    string path = "../../Databases/"; 
```
Важным моментом является то, что все базы данных Поляра являются строго типизованными. Это означает, что сначала надо создать объект класса PType, определяющий тип (желаемую структуру) данных, а потом можно сформировать в виде файла ячейку хранения значений данного типа. Предположим, наша задача состоит в записи некоторого текста в виде ячейки базы данных. Тогда тип будет:
```
    PType tp = new PType(PTypeEnumeration.sstring);
```
Здесь написано, что формируется тип tp, определяющий простую (системную) строку. Системная строка – это строка (string), полностью совместимая с .NET.

Следующий шаг – создание собственно ячейки для хранения (в файловой системе) значения данного типа:
```
    PaCell cell = new PaCell(tp, path + "test.pac", false);
```
Параметры здесь означают следующее: tp – тип, второй параметр – path к файлу-ячейке (с обязательным расширением .pac), третий параметр определяет ячейку как изменяемую. 

Теперь мы уже можем проверить некоторую работоспособность нашей программы (из двух операторов). При запуске программы, ничего нового выводиться не будет, однако, в директории Databases появится файл test.pac размером 32 байта. Это и есть Ваша база данных. 32 байта – некоторая служебная информация для ячейки. Последний из введенных операторов либо подключает Вашу программу к ячейке, если она есть, либо создает, если ее нет. Повторный запуск программы не произведет никаких изменений, поскольку теперь программа только подключится к test.pac.

Далее, записываем в ячейку данные заданного типа: 
```
    cell.Fill("Привет из ячейки базы данных!");
```
Пропустив программу и посмотрев на файл с ячейкой, обнаруживаем, что он «подрос» - в нем теперь записаны данные. Если попробовать снова выполнить программу, то получится Exception с указанием на то, что ячейка не пуста. Дело в том, что Fill заполняет только пустую ячейку, Чтобы ее «опустошить» можно уничтожить файл и при следующем пропуске программы, заведется новая ячейка. Но лучше опустошить (очистить) ячейку применив оператор Clear(), который рекомендуется поставить перед Fill. Кроме того, добавим чтение данных из базы данных, получим очищение – запись – чтение:
```
    cell.Clear();
    cell.Fill("Привет из ячейки базы данных!");
    Console.WriteLine("Содержимое ячейки: {0}", cell.Root.Get());
```
Если все было нормально сделано, выскочит то содержимое ячейки, которое мы записали. Закомментарив очищение и заполнение, новым выполнением программы, мы убедимся, что данные по-прежнему извлекаются из ячейки. 

Каждая ячейка предназначена для хранения значений определенного типа. До сих пор, мы работали только со строковым значением. Теперь рассмотрим использование записей, последовательностей и объединений, а также еще не рассмотренных атомарных типов. Создадим фрагмент вида:
```
    PType tp_rec = new PTypeRecord(
        new NamedType("имя", new PType(PTypeEnumeration.sstring)),
        new NamedType("возраст", new PType(PTypeEnumeration.integer)),
        new NamedType("мужчина", new PType(PTypeEnumeration.boolean)));
    object rec_value = new object[] { "Пупкин", 16, true };
    PaCell cell_rec = new PaCell(tp_rec, path + "test_rec.pac", false);
    cell_rec.Clear();
    cell_rec.Fill(rec_value);
    object from_rec = cell_rec.Root.Get();
    Console.WriteLine(tp_rec.Interpret(from_rec));
```
Здесь сначала определяется тип, представляющий собой запись из трех полей, строкового, целого и булевского типов. Потом определено константное значение rec_value, с конкретными значениями этих полей. Потом заполним ячейку этим значением, потом прочитаем в виде объекта значение, которое там хранится, потом интерпретируем прочитанное значение в контексте типа этого значения. Будет напечатано то, что вводилось, только в текстовом формате объектного представления. Подготовку объекта к печати в виде текста, осуществляет метод Interpret(v). 

Для лучшего понимания того, что написано в данных фрагментах, приведу выдержку из документации, связанную с объектной формой представления (напр. rec_value) структурных значений:

Объектная форма представления структурных значений. Структурные значения или их части (в силу рекурсивности построения структурных значений) могут существовать в виде объектов системы программирования. Вид такого представления - самый общий, это object. Атомарные значения представляются в виде значений соответствующих системных типов или классов: bool, char, int, long, double. Строки представляются строками. Составные (конструируемые) значения представляются в виде массивов объектов object[]. Причем для записи, элементы массива соответствуют полям записи, для последовательности, элементы массива соответствуют элементам последовательности, для объединения - массив состоит из двух элементов. Первый имеет целочисленное значение и соответствует индексу тега, второй элемент массива соответствует подзначению объединения. 

Итак, давайте сформируем более сложный тип, сформируем структурное значение, соответствующее этому типу, запишем значение в ячейку, т.е. в базу данных, потом прочитаем его из ячейки и распечатаем его в виде текста. Именно это мы делали в предыдущем фрагменте на примере простой записи. Будем работать с последовательностью записей. Соответственно, тип будет выглядеть:
```
    PType tp_seq = new PTypeSequence(tp_rec);
```
Мы воспользовались уже введенным типом tp_rec для того, чтобы определить последовательность записей. Объектное представление значения типа последовательности - массив объектов, представляющих записи. Пошлое значение объектного значения также будет расширено достаточно просто:
```
    object seq_value = new object[]
    {
        new object[] { "Иванов", 24, true },
        new object[] { "Петрова", 18, false },
        new object[] { "Пупкин", 22, true }
    };
    PaCell cell_seq = new PaCell(tp_seq, path + "test_seq.pac", false);
    cell_seq.Clear();
    cell_seq.Fill(seq_value);
    object from_seq = cell_seq.Root.Get();
    Console.WriteLine(tp_seq.Interpret(from_seq));
```
Решение совпадает с предыдущим один в один, с точностью до переименования. Но на этом решении обнаруживается ряд недостатков данного подхода. Главный - это не база данных, это хранилище отдельного структурного объекта. Мы его помещаем в ячейку, мы его извлекаем из ячейки. Но нет помещения по частям и извлечения по частям. Рассмотрим возможное решения задачи помещения по частям. Вот у нас в ячейке cell_seq есть три записи, а можно добавить четвертую? Можно. К PaCell последовательностям верхнего уровня применим метод добавления элемента AppendElement(r), где r - объектное представление добавляемого элемента. Эту часть реализуем как:
```
    cell_seq.Root.AppendElement(new object[] { "Сидоров", 23, true });
    Console.WriteLine(tp_seq.Interpret(cell_seq.Root.Get()));
```
Соответственно, можно частями (отдельными элементами) собрать уже большую последовательность, возможно не вмещающуюся в оперативную память. Теперь зададимся вопросом: как можно читать базу данный, в данном случае последовательность, по частям. Для ячеек реализован следующий подход: выделяется поле, в нем, если нужно выделяется подполе и т.д., в конце - производится выборка значения методом Get(). Методы выделения поля отличаются для разных составных типов. Для записи, это метод Field(nom), где nom - номер выделяемого поля, для последовательности, это метод Element(ind), где ind - номер выделяемого элемента, для объединения, это метод UElement(), дающий доступ к подэлементу. Есть еще получение свойств значений. Для последовательности, это метод Count(), дающий число элементов в последовательности, для объединения это метод Tag(), дающий текущий номер варианта типа для подзначения. Root - первичный выделитель, выделяющий все записанное значение.

Попрактикуемся на имеющейся последовательности из 4-х записей:
```
    long v0 = cell_seq.Root.Count();
    var v1 = cell_seq.Root.Element(2).Field(0).Get();
    var v2 = cell_seq.Root.Element(3).Field(1).Get();
    Console.WriteLine($"{v0} {v1} {v2}");
```
Важно понимать, что почти все простые значения (целые, логические и др.) фиксированного размера и только строки - нефиксированного. Все значения фикисроанного размера можно замещать другими значениями. Это делается через выделение поля и применения метода Set(), напр.: 
```
    cell_seq.Root.Element(1).Field(1).Set(19);
    cell_seq.Root.Element(1).Field(2).Set(true);
    Console.WriteLine(tp_seq.Interpret(cell_seq.Root.Get()));
```

## Task04_Sequenses: Последовательности и индексы
Последовательности играют ключевую роль в представлениях и в реализациях баз данных. В представлениях - потому что последовательность хорошо соответствует понятию "множество элементов таких, что...". Главное свойство последовательности - тип элементов. Кроме того, будучи отсортированной, последовательность представляет собой хороший способ дотсупа к искомому элементу через дихотомию (рекурсивное разбиение последовательности пополам). Вспомним также освоенный при изучении SQLite тест "Фототека", состоящий из трех связанных между сосбой таблиц.

Будем реализовывать последовательности, а тест "Фототека" будет давать сопоставление характеристик с другими системами и подходами.

Первую часть реализации мы уже проделали в Task03 - определение типов, определение ячейки, заполнение ячеки данными. Проделаем это еще раз, но теперь на (первой) таблице Фототеки, таблице персон. У нас будет целая череда решений,поэтому, для удобства, каждое новое решение будем оформлять как очередной Main(). 
```
using System;
namespace Task04_Sequenses
{
    partial class Program
    {
        static void Main(string[] args)
        {
            Main1(args);
        }
        public static void Main1(string[] args)
        {
        }
    }
}
``` 
Обратите внимание на модификатор partial. Это позволяет в другом файле снова определить статический метод Main2, Main3,... И просто поменять в Main() имя запускаемого метода. Можно было бы и без частичных определений классов, тогда вызов очередного теста выглядел бы Program3.Main3(args);

Добавим ссылки на Поляр, определим тип.  
```
    Console.WriteLine("Start Task04: Main1");
    PType tp_rec = new PTypeRecord(
        new NamedType("id", new PType(PTypeEnumeration.integer)),
        new NamedType("name", new PType(PTypeEnumeration.sstring)),
        new NamedType("age", new PType(PTypeEnumeration.integer)));
    PaCell cell = new PaCell(new PTypeSequence(tp_rec), "../../Databases/" + "people.pac", false);
```

В режимк ввода, ячейку очищаем, заполняем пустой последовательностью, а далее в цикле заполняем записями, соответствующими тесту Фототека. Напр. так:
```
    int npersons = 1_000_000;
    Random rnd = new Random();
    Console.WriteLine("Start Task04: Main1");
    bool toload = true;
    
    if (toload)
    {
        sw.Restart();
        cell.Clear();
        cell.Fill(new object[0]);
        for (int i=0; i<npersons; i++)
        {
            int code = npersons - i;
            cell.Root.AppendElement(new object[] { code, "=" + code + "=",  rnd.Next(120)});
        }
        cell.Flush();
        sw.Stop();
        Console.WriteLine($"load {npersons} records. duration {sw.ElapsedMilliseconds}");
    }
```
Загрузка 1 млн. записей выполняется около 430 мс. Это существенно лучше, чем демонстрировал SQLite. Теперь надо организовать выполнение выборки записей по заданному ключу (идентификатору). Как уже было показано, выборка элемента осуществляется:
```
    var r = cell.Root.Element(i).Get();
```
Проблема заключается в том, что для последовательностей с нефикированным размером, метод Element(i) хоть  и определен, но выполняется перебором элеменов Element(0), Element(1),... Кроме того, мы не знаем какой индекс соответствует ключу-образцу. 

Вопользуемся следующим обстоятельством: класс указателя, получающегося в результате cell.Element(i) имеет структуру, который дествительно указывает на i-ый элемент, но в котором можно подменить offset и тогда он может указывать на другой элемент.
```
    PaEntry entry = cell.Root.Element(i);
    entry.offset = anotheroffset;
    object r = entry.Get();
```
Для построения эффективной схемы организации доступа по ключу, построим вспомогательный массив пар {key, offset}, в котором пара вычисляется по элементу последовательности - key будет ключом (идентификатором) элемента, а offset - смещением элемента в потоке байтов. После этого, отсортируем этот массив по значению ключа и применим бинарный поиск для выявления нужной пары, а уже по ней, определим offset и по нему найдем элемент.    

Эти естественные действия имеются в библиотеке Polar.Cells, чем мы и воспользуемся. Итак, в раздел описания типов и ячеек, добавится описание ячейки для индекса:
```
    PType tp_pair = new PTypeRecord(
        new NamedType("id", new PType(PTypeEnumeration.integer)),
        new NamedType("offset", new PType(PTypeEnumeration.longinteger)));
    PaCell cell_index = new PaCell(new PTypeSequence(tp_pair), path + "people_index.pac", false);
```
Фрагмент загрузки изменится на:
```
    cell.Clear();
    cell.Fill(new object[0]);
    cell_index.Clear();
    cell_index.Fill(new object[0]);
    for (int i = 0; i < npersons; i++)
    {
        int code = npersons - i;
        long offset = cell.Root.AppendElement(new object[] { code, "=" + code + "=", rnd.Next(120) });
        cell_index.Root.AppendElement(new object[] { code, offset });
    }
    cell.Flush();
    cell_index.Flush();
    cell_index.Root.SortByKey<int>(pair => (int)((object[])pair)[0]);
    sw.Stop();
    Console.WriteLine($"load {npersons} records. duration {sw.ElapsedMilliseconds}");
```
А тестирование доступа к записям по задаваемому ключу может выгдядеть как:
```
    int key = npersons * 2 / 3;
    int ntests = 10000;
    sw.Restart();
    PaEntry entry = cell.Root.Element(0);
    for (int j = 0; j < ntests; j++)
    {
        key = rnd.Next(npersons);
        PaEntry en = cell_index.Root.BinarySearchFirst(ent => ((int)((object[])ent.Get())[0]).CompareTo(key));
        object operson = entry.SetOffset((long)((object[])en.Get())[1]).Get();
        //Console.WriteLine($"val={tp_rec.Interpret(operson)}");
    }
    sw.Stop();
    Console.WriteLine($"getByKey {ntests} times. duration {sw.ElapsedMilliseconds}");
```
Ключевым оператором фрагмента является получение ссылки на первый элемент BinarySearchFirst() в последовательности, удовлетворяющий некоторому условию. Последовательность должна быть отсортирована, а условие выражается функцией сравнения с образцом.

Загрузка, естественно, несколько замедлится, у меня это 1200 мс. на миллион записей, а 1000 запросов по ключу выполняется за 50 мс. Это хуже, чем в самом быстром случае демонстрирует SQLite (35 ms), но не намного. И это для нас "вызов": сделать выборки быстрее.  

В пространстве имен Polar.CellIndexes имеется обеспечение построения индексов, мы познакомимся с этими средствами во второй части практического занятия. 

### Вначале немного теории
Индексом к некоторому поисковому полю (к структуре данных) называется дополнительная структура, позволяющая экономно выполнять операции поиска по какому-то признаку или сочетанию признаков. В этом плане, возможность введения типа «последовательность чего угодно», мы заменяем на "последовательность элементов типа T", на языке Поляр это выглядит как OurType = [T], в функциональном базисе PType это же выглядит как PType ourType = new PTypeSequence(t), где t – ранее определенный тип (PType-значение). 
Что мы умеем делать в этой постановке? Мы умеем заводить ячейку, заполнять последовательность произвольным количеством значений, ну и кое-что еще. Представим, что полученная ячейка (база данных) довольно большая (напр. миллиард элементов), а нам требуется выполнять поиск. Предположительно, перебор элементов для осуществления поиска неэффективен. Нужно создавать индекс. 

Поставим задачу несколько более формально. У нас есть множество M элементов e. Кроме того, имеется (часто бесконечное) множество S (Samples) элементами которого являются образцы s. На элементах MxS определен предикат P(e, s). Поисковая задача состоит в том, чтобы для конкретного s0 выделить подмножество V такое, что на его элементах P(s0, e) было истинным, на остальных – ложным. Понятно, что в общем случае, задача решается перебором элементов множества M и вычислением на этих элементах предиката P. Но есть важные частные случаи. Например, если предикат P(e, s) сводится к форме Q(e) = R(s), где функции Q и R имеют общее множество значений, причем это множество – упорядоченное. Тогда выстроим элементы e по увеличению значения Q(e) (выполним сортировку). Теперь если появляется поисковый образ s0, то, вычислив R(s0), мы будем иметь возможность найти все элементы удовлетворяющие Q(e) = R(s0) за O(Log2(N)) вычислений Q(e). Собственно значения Q(e) можно не хранить. Важно заранее упорядочить элементы по значению этой функции. 

Поскольку мы не хотим переупорядочивать элементы базовой последовательности, базовым индексным решением является последовательность пар: {значение ключа элемента, координата (offset) элемента}. Собственно именно такое решение было продемонстрировано в предыдущем проектировании. Этоть вариант "работает" в случае, когда значение ключа имеет фиксированный размер. В противном случае, напр. если ключ - строка, так уже не получится. Для такого варианта, индексное решение может быть просто последовательностью координат (offset'ов) элементов. В этом случае, первый элемент пары не хранится, а вычисляется по заданному смещению. 

В пространстве имен Polar.CellIndexes воспроизведена следующая логика. Основным объектом является опорная последовательность. Часто, для отличия, ее называю таблицей. Это потому, что типичным вариантом опорной последовательности является последовательность записей, а значит - таблица. 
Таблица создается как объект класса ViewTable, при его задании определяется тип элементов таблицы (последовательности). Индексы к таблице содаются по следующей схеме: 
1) Создается один из трех видов индекса, эти индексы являются неизменяемыми (Immutable). Это выполняется с использованием одного из классов: IndexKeyImmutable, IndexViewImmutable, IndexHalfkeyImmutable. При создании индексов задается опорная таблица и функция вычисления ключа. 
2) На базе "неизменяемого" индекса создается "изменяемый" класса DynamicIndex. Такая трансформация позволяет динамически отслеживать изменения в опорной последовательности. 
3) Полученный индекс регистируется в опорной таблице. Это позволяет распространять на произвольное множество индексов данной последовательности такие методы, как построение (перестроение). Также изменения в опорной таблице автоматически сопровождаются изменениями во всех зарегистрированных индексах.

О различии в разных индексах (IndexKeyImmutable, IndexViewImmutable, IndexHalfkeyImmutable). Самый естественный и быстрый - IndexKeyImmutable. Это простой набор значений ключ-offset, отсортированных по значению ключа, описанный ранее. Индекс применим к ситуации, когда ключ имеет фиксированный размер. Самый общий вариант - IndexViewImmutable, в нем в индекс реализутся последовательностью офсетов, отсортированных по значению ключевой функции, вычисленной на элементе опорной последовательности, на который указыват офсет.

Второй вариант имеет тот недостаток, что и при сортировке и при поиске за каждым значением ключа приходится "заглядывать" дважды - сначала в индексный массив (последовательность), потом в опорную таблицу. Третий вариант в ряде случаев позволяет преодолеть эту неэффективнойсть. Этот метод использует понятие "полуключа". Суть подхода заключается в том, что для ключа (напр. строки) задается какая-то функция, напр. хеш-функция, переводящая ключ в значение фиксированного размера - в полуключ. Это позволяет сделать последовательность пар полуключ-офсет. Далее, все происходит почти также, как с ключем за исключением того, что совпадение полуключей не гарантирует равенства ключей, поэтому для совпадающих полуключей приходится проводить дополнительную проверку на совпадение ключей. 

Новое определение таблицы и первичного ключа будет выглядеть следующим образом:
```
    TableView tab_person = new TableView(path + "person", tp_person);
    Func<object, int> person_code_keyproducer = v => (int)((object[])((object[])v)[1])[0];
    IndexKeyImmutable<int> ind_arr_person = new IndexKeyImmutable<int>(path + "person_ind")
    {
        Table = tab_person,
        KeyProducer = person_code_keyproducer,
        Scale = null
    };
    IndexDynamic<int, IndexKeyImmutable<int>> index_person = new IndexDynamic<int, IndexKeyImmutable<int>>(true)
    {
        Table = tab_person,
        IndexArray = ind_arr_person,
        KeyProducer = person_code_keyproducer
    };
    tab_person.RegisterIndex(index_person);
```
Загрузку и построение индекса выполним не так, как раньше, но аналогично:
```
    IEnumerable<object> flow = Enumerable.Range(0, nelements)
        .Select(i =>
        {
            int id = nelements - i;
            string name = "=" + id.ToString() + "=";
            double age = rnd.NextDouble() * 100.0;
            return new object[] { id, name, age };
        });
    tab_person.Fill(flow);
    // Теперь надо отсортировать все индексы
    tab_person.BuildIndexes();
```
Доступ к элементам по индексу: 
```
    int search_key = nelements * 2 / 3;
    var ob = index_person.GetAllByKey(search_key)
        .Select(ent => ((object[])ent.Get())[1])
        .FirstOrDefault();
    if (ob == null) throw new Exception("Didn't find person " + search_key);
```
Организуя случаное назначение ключа и выполняя этот фрагмент в цикле, можно измерить производительность реализованного решения. У меня загрузка миллиона записей получилась 2.3 сек., а выполнение 1 тыс. доступов - 53 мс. Это вполне сопоставимо с результатами SQLite на аналогичном тесте. Время выполнения доступов по ключу можно существенно уменьшить если добавить к решению так называемую шкалу Scale. Суть решения в том, что поиск по ключу надо производить в отсортированной по значению ключа последовательности. Стандартно, это требует применения метода дихотомии и, соответственно, ln N обращений к элементам последовательности. Если значения ключа более или менее равномерно распределены по некоторому интервалу, то можно этот интервал разбить на равные отрезки и по каждому отрезку подсчитать количество значений последовательности, попавших в этот отрезок. Соответственно, легко организовать массив (или последовательность) позволнящий по значению ключа в одно действие определять отрезок, в который ключ попал, определять начальный индекс подряд идущих значений последовательности и их число для проверки на совпадение ключа. Вся эта процедура реализована в виде ячейки-шкалы ScaleCell, легко подсоединяемой к ключевому индексу дейсвием:
```
    ind_arr_person.Scale = new ScaleCell(path + "person_ind") { IndexCell = ind_arr_person.IndexCell };
```
Это делается перед тем, как "неизменяемый" индекс используется для создания динамического индекса. Испытание такого решения показывает, что на время загрузки оно практически не влияет, а время выполнения доступов существенно уменьшает. Теперь оно стало 18 мс. на 1 тыс. запросов. 

### Индекс для ключей переменной длины

Если ключ в последовательности имеет нефиксированный размер, напр. он строка, используется IndexViewImmutable. В принципе, почти все как для предыдущего примера. Создается "неизменяемая" часть индекса с использованием IndexViewImmutable. Также указываются опорная таблица и ключевая функция. На основе этого объекта, строится объект класса DynamicIndex. Этот индекс регистрируется в опорной таблице. Приведу возможные строки кода, полный вариант программы размещен в Program3.      
```
    IndexViewImmutable<string> name_index = new IndexViewImmutable<string>(path + "name_index.pac")
    {
        Table = tab_person,
        KeyProducer = v => (string)((object[])((object[])v)[1])[1]
    };
    IndexDynamic<string, IndexViewImmutable<string>> index_person_name = new       IndexDynamic<string, IndexViewImmutable<string>>(false, name_index);
    tab_person.RegisterIndex(index_person_name);
```
Рассмотренный метод, при всей своей универсальности обладает недостатком - он 
медленный. Любое действие со значением ключа требует выборки из таблицы офсетов,
потом полной выборки записи из другой таблицы. И такие действия выполняются на 
каждом шаге бинарного поиска. Существует подход, который корректирует ситуацию. 
Все было гораздо лучше для случая наличия ключа. Тогда, для тех же действий по 
бинарному поиску, на каждом шаге поиска требовалось "заглядывать" только в одну
таблицу. Чтобы получилось также, введем функцию с частичными свойствами ключевой
функции. То есть, пусть на элементах опорной последовательности определена функция,
выдающая сравнымый результат фиксированного размера. Например, на поле строкового
типа можно вычислять целочисленное хеш-значение. Важно, чтобы функция максимально
"разбрасывала" значения по диапазону. Почему полуключ? Потому что ключ дает взаимно
однозначное соответствие между значением записи в последовательности и целочисленным кодом в каком-то интервале, а полуключ этого не делает. Разные записи могут давать одинаковые значения кода полуключа. Соответственно индексная последовательность 
строится на основе двухэтапной сортировки - сначала по полуключу, а потом по ключу опорной последовательности. Выигрыш заключается в том, что сравнение двух элементов, как правило, заканчисается на сравнении полуключей и только в случае совпадения, выполняется более сложное основное сравнение. 

Технически, перейти от view-индекса к полуключевому, довольно просто - заменяется только определение индексного объекта и вместо оператора IndexViewImmutable<string> name_index = ... определяется его полуключевой аналог:
```
    IndexHalfkeyImmutable<string> name_index = new IndexHalfkeyImmutable<string>     (path + "name_hindex.pac")
        {
            Table = tab_person,
            KeyProducer = v => (string)((object[])((object[])v)[1])[1],
            HalfProducer = s => Hashfunctions.HashRot13(s)
        };
```
Обратите внимание на то, что добавился еще один параметр HalfkeyProducer - задание функции вычисления полуключа из ключа. В примере она задается через хеш-функцию.

Еще один нюанс заключается в том, что теперь снова можно использовать шкалу задав 
соответствующий параметр также как и для "стандартного" ключевого индексного построения IndexKeyImmutable. Повторим ранее использованный для этого фрагмент:
```
    name_index.Scale = new ScaleCell(path + "person_ind") { IndexCell =        name_index.IndexCell };     
```
Итак, для работы со строкой, как ключем для поиска (на моем компьютере) у нас yf 1 млн. элементов получились следующие результаты (время загрузки, время поиска):
IndexViewImmutable 5 сек., 188 мс. / 1000 выборок
IndexHalfKeyImmutable 3.3 сек., 82 мс
IndexHalfKeyImmutable + Scale 3.4 сек., 19 мс

Последний результат уже неплох!

Также обратите внимание на повторный запуск поисков, запуск без загрузки данных. Из уже полученного опыта, мы знаем, что хорошо бы иметь "разогрев" базы данных. За счет индексных построений, основная таблица и ее индексы, разогреваются единым запуском:
```
    tab_person.Warmup();
```
Можно поэкспериментировать на размерах данных хотя бы в 10 млн. элементов. У меня после холодного запуска 1000 поисков выполнялись 80 сек., а если холодный запуск сочетать разогревом, 1000 поисков выполняется 65 мс., правда разогрев длится 30 сек.

## Элементы функционального программирования
С каких-то пор в C# появился слой функционального программирования LINQ - Language INdependent Queries. Он был введен как способ формулирования запросов к базам данных. Однако, его значимость и для языка и для обработки данных существенно шире.
У LINQ есть функциональная нотация и есть специфическая, языковая, похожая на SQL. Мы пользуемся только функциональной нотацией. 

Ключевой для слоя LINQ является парадигма потока и потоковой обработки. Суть его близка к теоретико-множественным построениям, поэтому имеет глубокие корни. Введем поток как абстракцию последовательности элементов одного типа T. Поток может быть порожден разными способами, может быть преобразован, может быть превращен в объект. C# тип потока или перечисления IEnumerable<T>. Порождение потока возможно из массивов, списков и др. объектов, имеющих интерфейс IList, ICollection или	IEnumerable. Есть специальные статические методы класса Enumerable, позволяющие генерировать простые потоки однотипных значений. 

Преобразования потоков выполняется рядом преобразователей, среди которых наиболее принципиальными являются Select - функциональное преобразование и Where - фильтрация. Использование потоков выполняется двумя основными способами - организацией цикла foreach и пробразовнанием потока в значение. Рассмотрим пример:  
```
    int[] arr = { 9, 8, 7, 6, 5, 4, 3, 2, 1, 0 }; // массив целых со значением
    var query = arr.Where(x => x % 2 == 0).Select(x => x * x).OrderBy(y => y);
    // Использование запроса в цикле
    foreach (var v in query) Console.Write($"{v} ");
    Console.WriteLine();
    // Использование запроса для вычисления значений
    int[] arr2 = query.ToArray();
    int ind = 2;
    Console.WriteLine($"element of array arr2 with index {ind} is {arr2[ind]}");
```
Здесь ключевым является 2-й оператор. В нем определяется запрос в виде Linq-выражения. В выражении массив arr (автоматически) превращается в поток, который фильтруется с оставлением только четных значений, потом элементы потока возводятся в квадрат, потом сортируются. IEnumerable<T> - перечисление, определение запроса не выполняет вычислений - это всего лишь формула. Далее, выражение используется для построения цикла с перебором значений. Здесь формула уже вычисляется, результат распечатывается. Но запрос (в данном случае - query) можно использовать для вычисления объекта и для перевода его, например, в массив. Заметим, что query 7-й строчки вычисляется повторно. 

В наборе операций преобразования последовательностей входят: конкатенация, группирование, "любимый" по SQL join. Имеется набор средств работы с потоками как со множествами: слияние одинаковых, объединение, пересечение, разность. Есть средства генерации первичных потоков, есть предикаты Any, All, Contains. Есть ключевое для технологий MapReduce преобразование Aggregate, разные моды и варианты общих и частных действий.

В каком-то смысле, LINQ - это целый мир и изучать его надо серьезно. Имеется много учебных пособий, позволю себе рекомендовать сайт https://professorweb.ru/my/LINQ/base/level1/linq_index.php

## Task05_XML: стандартные представления структурированных данных 
XML - наиболее известный и широко распространенный язык и средство иерархического структурирования данных. XML в узком смысле, это язык разметки, т.е. текстовое средство представления иерархически устроенных данных. В широком смысле, XML это система стандартов, представлений и средств специфической структуризации данных.  

Имеется 2 основных способа работы с XML-файлами: сканирование с вызовом процедур-обработчиков (handlers) и загрузка документа в оперативную память в виде объекта. Одно такое внутреннее представление называется DOM (Document Object Model), оно стандартизировано и используется в многочисленных системах и технологиях. Мы освоим другое внутреннее представление Linq to XML с его реализацией в библиотеке .NET Framework. В этом представлении XML-объект рассматривается как источник потоков, а также он может быть собран средствами Linq-формулы так, как это будет показано далее. 

Для наглядности, начнем с XML-документа. Зададим в виде XML простую базу данных, соответствующую таблице persons набора "Фототека". Что-нибудь простое, напр.:
```
<?xml version="1.0" encoding="utf-8" ?>
<db>
  <person id="p001">
    <name>Иванов</name>
    <age>22</age>
  </person>
  <person id="p002">
    <name>Петров</name>
    <age>23</age>
  </person>
  <person id="p003">
    <name>Сидоров</name>
    <age>21</age>
  </person>
  <person id="p004">
    <name>Пупкин Вася</name>
    <age>55</age>
  </person>
</db>
```
Добавим средствами Добавить/Создать элемент/XML-файл в проект новый пустой элемент, в редакторе, доавим в этот файл базу данных. Первая задача: загрузить файл в виде XML-объекта в программу. Средства работы с XML расположены в пространстве имен System.Xml.Linq, соответственно добавим
```
using System.Xml.Linq;
```
А в код вставим оператор загрузки из файла и оператор проверки того, что значение загружено:
```
    XElement db = XElement.Load("XMLdata.xml");
    Console.WriteLine(db.ToString());
```
Теперь давайте поробуем сделать выборку из такой базы данных. XElement как источник (потоков) данных имеет несколько направлений или "осей" (axis) выборки частей хранимого дерева. Для наших целей, подходит нпаравление подэлементов. Соответственно, запрос на поток подэлементов корневого элемента db, создается db.Elements(). Пусть у нас задан идентификатор искомого элемента. Запрос организуем как сформированный из db потока элементов, потом отфильтруем те, которые содержат атрибут id с нужным идентификатором, потом выберем первый, потом снова запросим подэлементы найденной записи, а элементы сагрегируем в единый текст, который распечатаем. 
```
    string id = "p002";
    var q = db.Elements()
        .Where(el => el.Attribute("id").Value == id)
        .First()
        .Elements()
        .Select(e => e.Value)
        .Aggregate((s, t) => s + " " + t);
    Console.WriteLine(q);
```
Писать Linq-формулы с непривычки довольно сложно, но если постоянно тренироваться, скоро без Linq вы уже не сможете комфортно себя ощущать.

Теперь рассмотрим процесс констрирования XML-объекта из частей. Причем процесс является рекуррентным, а сформированные формулы похожи на "классичские" лесенки глубоко вложенных LISP-выражений. Основой таких конструирований являются конструкторы с переменным числом параметров. Например, для создания XML-элемента, применяется конструктор new XElement("tagname", ...), где многоточие - место для произвольного числа внутренних узлов и атрибутов.

Для того, чтобы произвести процедурное формирование аналогичного db объекта, нужен фрагмент типа:   
```
    var db2 = new XElement("db",
        new XElement("person",
            new XAttribute("id", "p001"),
            new XElement("name", "Иванов"),
            new XElement("age", "22")),
        new XElement("person",
            new XAttribute("id", "p002"),
            new XElement("name", "Петров"),
            new XElement("age", "23"))
        // ...
        );
```
Вот и "лесенка" появилась! Но чистый повтор статического формирования базы данных динамическими конструкциями нам не сильно интересен. Мы можем пойти дальше. Дело в том, что в конструкциях множества однотипных параметров, можно использовать поток элементов данного типа и это качественно улушает возможности. Теперь напишем простой генератор подобной тестовой базы данных, но уже произвольного размера:
```
    int npersons = 1_000_000;
    Random rnd = new Random();
    XElement db3 = new XElement("db",
        Enumerable.Range(1, npersons)
        .Select(i => new XElement("person",
            new XAttribute("id", "p" + i),
            new XElement("name", "=p" + i + "="),
            new XElement("age", "" + rnd.NextDouble() * 150))));
    foreach (var el in db3.Elements().Skip(1000).Take(5))
    {
        Console.WriteLine(el.ToString());
    }
```
В данном фрагменте мы формируем в оперативной памяти объект db3, состоящий из 1 млн. элементов уже использованной структуры. Конструкция Enumerable(start, number) генерирует поток подряд стоящих целых чисел от start в количестве number. Констукции Skip(number1) и Take(number2) пропускают и соответственно выдают number первых значений потока. Сформированную базу данных можно записать в файл простым действием db3.Save(имя_файла);

## Task06_Web: Простое Web-приложение
Теперь мы "дозрели" создания простого серверного приложения. В качестве базы данных возьмем XML-базу, сформированную в 5-м задании db3.xml. Задача Web сервера (сервиса) будет выдавать клиенту запись базы данных по ее идентификатору id.

В технологии .NET Core, с привлечением технологий XML и LINQ это не так сложно, что и будет продемонстрировано в данном задании. Задание начнем в (уже подзабытой) инструментальной системе Visual Studio Code. 

Запустим приложение Visual Studio Code, откроем директорию Вашего решения
Запустим интерпретатор командной строки, есть встроенный интерпретатор в Visual Studio Code - очень удобно. Создадим директорию для проекта, напр. Task06_Web, и (новшество!) создадим новый проект только по другому шаблону:
```
dotnet new web
```
Появились две программы, что они значат - нам пока не так важно, но примитивное (пустое) Web-приложение уже сформировано. Запустим программу dotnet run. Пускач написал, что установил сервер (слушателя) на порт 5000. Через браузел посмотрим что там "висит". Получили "Hello World!". Нетрудно убедиться, что это простая текстовая строка и увидеть место, где она порождается. Заменим текстовую строку на что-нибудь вроде
```
"<html><body><h1>Старт Task06_Web</h1></body></html>"
```
Изменения на странице не произошло, что не удивительно - приложение уже запущено, его надо сначала закрыть, потом скомпилировать и запустить (оба действия выполняются при dotnet run). Убедимся, что мы получили "настоящую" web-страницу, пришедшую от приложения в виде HTML. 

Теперь поставим задачу: сделать из этого приложения визуализатор базы данных db3.xml, сформированной в предыдущем задании. Напомню, что в виде XML-документа зафиксированы записи о персонах, каждая запись содержит индентификатор персоны, имя персоны, возраст персоны. 

Теперь для большего удобства разработки, перейдем в VisualStudio. Там просто добавим к Вашему решению этот начальный вариант проекта. Можно даже не закрывать Visual Studio Code. Запустим проект, можно заметить, что он запускается без видимой консоли командной строки и также через какой-то технический порт и сразу с запуском (дефолтного) браузера. Еще одной приятной особенностью такого способа работы с проектом является то, что в большинстве случаев, изменение кода серверного приложения "автоматически" отрабатывается вплоть до картинки в браузере. Можно это проверить тем, что немного изменить код программы формирования респонса и потом просто обновить картинку в браузере. Судя по задержки в появлении новой картинки, запрос к серверному приложению вызывает компиляцию, отработку запроса с формированием ответа в новом варианте приложения. Если текс программы не менять, то обновление страницы осуществляется очень быстро. В некоторых случаях, однако, приходится останавливать приложение для того, чтобы сработало обновление. В простом режиме dotnet run было достаточно остановить его нажав <cntrl>C, здесь нужно найти запустившую административную систему и явно остановить приложение. 

Теперь нам нужно прочитать базу данных. Добавим для Startup.cs пространство имен System.Xml.Linq и перед app.Run выполним XElement.Load().
```
    XElement db = XElement.Load("../../Databases/db3.xml");
```
а оператор await context.Response.WriteAsync(...) преобразуем в:
```
    XElement html = new XElement("html", 
        new XElement("head", new XElement("meta", new XAttribute("charset", "utf-8"), " ")),
        new XElement("body",
            new XElement("h1", "Start Web test"),
            new XElement("div", "DateTime: " + DateTime.Now.ToString()),
            new XElement("div", "Всего элементов: " + db.Elements().Count()),
            new XElement("div", "id=" + context.Request.Query["id"]),
            null));
    await context.Response.WriteAsync(html.ToString());
```
Этот фрагмент определяется как тело определения функции (без результата) на вход которой подается контекст, в частности Request (запрос) и его параметры, а косвенным результатом является Response, отправляемый браузеру. Обращаю внимание на то, как вычисляется значение строкового идентификатора id, возможно присутсвующего в запросе, если его нет, то у выражения context.Request.Query["id"] будет значение null.

Теперь сформулируем то, что мы хотим, чтобы серверное приложение делало. В качестве постановки задачи мы зададим следующее: если в параметрах запроса нет id, то выдается часть базы данных списком. Элементы списка соответствуют записям и в визуализации есть гиперссылка с запуском визуализатора сконкретным id. Если id задан, то выдаются поля записи с этим идентификатором. специфической особенностью задания является то, что html-ответ надо (для учебных целей) сформировать единым LINQ-выражением (более сложным, чем приведено рассмотренное ранее). Задача для новичка не простая, но полезная - чем лучше вы будете владеть LINQ, тем больше у вас будут возможности написания наглядного и эффективного кода.     

Какие можно сделать выводы из проделанной работы и созданного Web-приложения? Во-первых, вы познакомились со специфической технологией написания Web-приложений, она оптимизирована по значительному числу направлений и позволяет не слишком искушенному в тонкостях сетевых решений программисту писать достаточно эффективные серверные приложения. Эта технология работает не только в Windows, но и в Linux и iOS. Во-вторых, вы ощутили мощь фукционального программирования. В-третьих, еслиеще дополнительно поэкспериментировать, то выяснится, что технологии XML вполне годятся для решения задач хранения структурированной информации и предоставлении к ней быстрого доступа. Обычно, даже для простых приложений, рекомендуется разворачивать SQL-базу данных. Можно легко увидеть, что прямое использование XML позволяет создавать достаточно "реактивные" приложения, прада это касается случаев, когда база данных имеет небольшой объем. Этот "небольшой" объем в современных технологиях это базы данных с десятками миллионов записей, что как правило, значительно превышает потребности большинства информационных систем. 

### Задание для продвинутых 
Есть смысл потратить дополнительные усилия для продолжения задания. Можно это сделать в следующих направлениях:
- расширение базы данных и ее визуализации до трех таблиц теста "фототека";
- "украшение" средствами HTML получающихся страниц;
- перевод базы данных на поляровские ячейки;
- использование "своих" данных вместо сгенерированных, построение "своего" генератора.

Некоторые рекомендации. Базу данных удобнее формировать в виде:
```
<db>
  <!-- произвольная последовательность элементов вида: -->
  <person id="p12345">
    <name>Иванов ...</name>
    <age>...</age>
  </person>
  <foto id="f00001">
    <name>DSP38747</name>
  </foto>
  <reflection id="r11837">
    <reflected ref="p12345"/>
    <infoto ref="f00001"/>
  </reflection>
  ...
</db>
```
Украшение надо осуществлять не в сторону цветов, шрифтов и т.д., а в сторону лаконичной, но понятной визуализации именно того, что нужно.

## Task07_MongoDB
MongoDB - довольно известная СУБД класса NoSQL. Когда говорят о MongoDB, обычно отмечают, что это документо-ориентированная база данных. Это не совсем правильно. Документ в это системе - это JSON-объект, можно сказать - JSON-запись. Правильнее MongoDB классифицировать как key-value хранилище безтиповых JSON объектов с формированием дополнительных индексов, включая - вектрные индексы.

Установка MongoDB осуществляется через инсталятор. Либо, кажется, можно скопировать директорию и использовать ее. Система и документация к ней находится на сайте https://www.mongodb.com/
Я беру пакет по ссылке https://www.mongodb.com/download-center в разделе Community Server в подразделе All Version Binaries и там версию 3.6.4 (файл win32/mongodb-win32-x86_64-2008plus-ssl-3.6.4.zip). Архив надо развернуть - это готовая к использованию система. Если Вы будете этой системой пользоваться, то можно существенно уменьшить ее объем уничтожив все .pdb файлы. 

Систему надо куда-то установить, создать директорию для данных (по умолчанию это \data\db) и запустить приложение mongod.exe - это серверное приложение (сервис), поддерживающее базу данных и обслуживающее клиентов. Перед запуском приложения, хорошо бы почитать краткие инструкции: ./mongod.exe -h. А уже потом, что-нибудь вроде
```
./mongod.exe --dbpath path-to-data
```
Серверное приложение будет запущено и можно начинать его осваивать. Полезно почитать на сайте документацию. Но ее много, а хочется что-то сделать быстро. Попробуем быстро. Есть консольный клиент, запускается через mongo.exe. Еще (вроде) есть Web-вход, он обычно используется для администрирования. Пока воспользуемся консольным клиентом. Запустим его в консоли, увидим, что он присоединился к серверному приложению, начинаем работать. Набрав help увидим возможные команды. На пока нужны совсем немногие:
```
show dbs
use mydatabase
show collections
```
Первый оператор показывает какие базы данных, содержатся в области данных. Второй оператор, определяет ту базу данных, с которой пользователь собирается работать. Если такой нет, она автоматически заводится. База данных содержит коллекции. Посмотреть коллекции (той базы данных, которая используется) можно тертим оператором. Пока в нашей новой базе данных коллекций нет. Работа с коллекциями выполняется, в основном операторами, начинающимися с 
```
db.collection1.что-нужно-сделать
```
Это "что-нужно-сделать" можно узнать через db.collection1.help(). Опять же все нам не нужно, обратим внимание на добавление insert() в коллекцию нового элемента и на поиск элементов по образцу find(). Они достаточно простые и понятны из приведенного примера:
```
db.collection1.insert({name:"Иванов", id:499001, age: 23});
db.collection1.insert({name:"Петров", id:499002, age: 24});
db.collection1.insert({name:"Пупкин", id:499003, age: 25});
db.collection1.insert({name:"Иванов", id:499004, age: 26});
db.collection1.find();
db.collection1.find({name:"Иванов"});
db.collection1.find({id:499004});
```
Обратите внимание на то, что одинаковые Ивановы различаются и системным идентификатором и "нашим" идентификатором и мы по нему можем легко апросить нужного. 

Можете "играться" дальше, в документации по mongoDB есть и примеры и объяснения.

Теперь мы будем создавать своего клиента. Идея здесь такова: нам нужна база данных, мы берем чужую и общаемся с ней через адаптер. Адаптер обычно можно найти либо в составе используемой СУБД, либо где-то еще. Нас будут интересовать адаптеры, позволяющие работать с СУБД из программ на C# (.NET Core).  

Создадим консольный Core проект и воспользуемся NuGet для подкачки драйвера, в проектный конфигуратор вставим:  
```
  <ItemGroup>
    <PackageReference Include="MongoDB.Driver" Version="2.5.0" />
    <PackageReference Include="MongoDB.Driver.Core" Version="2.5.0" />
    <PackageReference Include="MongoDB.Bson" Version="2.5.0" />
    <PackageReference Include="System.Reflection.Emit.Lightweight" Version="4.3.0" />
  </ItemGroup>
```
Или сделаем добавление пакетов средствами PM Visual Studio. Далее, при запущенном приложении mongod, будем коннектиться к нему, объявлять коллекцию, заполнять коллекцию, осуществлять поиск элементов по коллекции.

Внимательно прочитайте текст программы данного проекта и попробуйте его понять. Если у Вас возникли вопросы, прочитайте следующие за этим абзацем пояснения.

Строчки 3-5 добавляют соответствующие пространства имен. Предполагается, что NuGet пакеты загружены. Строчка 2 - подключение базовой бибилиотеки LINQ. 

Строчка 16: подключение данной программы к серверу, запущенному на этой машине и "висящем" на указанном порту. 

Строчка 23: Если мы производим загрузку данных, то сначала уничтожается коллекция, которая была под этим именем и возможно заполнялась в предыдущих запусках. 

Строчки 46-47: Для ускорения поиска по find(), мы строим два индекса. Один для поиска по (нашему) идентификатору, другой - для поиска по имени.

Строки 68, 78: поисковый образ строится как фильтр, работающий на "равенство", берется первый найденный результат. 

### Task08_JSON - работа с форматом JSON
Для структурирования данных применяется множество подходов и форматов. Некоторые из них становятся стандартами и важны для оббеспечения совместимости Ваших программ и программ, написанных кем-то. Возможно второй по популярности формат структурирования данных, это JSON - Java Script Object Notation. Он позволяет формировать достаточно произвольные иерархические структуры и в этом похож на XML, но JSON - более компактный и напрямую может быть использован в JavaScript программах и в этом многие видят его преимущество. JSON - текстовый формат, поэтому его можно формировать и редактировать "вручную", также как и генерировать программно. 

Как устоен JSON? Мы уже в какой-то мере работали с JSON через MongoDB. Теперь чуть поподробнее. 

JSON-объекты строятся рекуррентно (или рекурсивно, если Вам так понятнее). Это значит, что JSON-объект это или примитивный объект: число, булевское значение, строка и др. Или это композиция одного из двух видов: последовательность JSON-объектов или запись JSON-объектов. Последовательность изображается группированием элементов в квадратных скобках, а запись - группированием в фигурных скобках. При этом, каждому элементу записи придается стороковое значение, часто используемое в качестве идентификатора поля. Примером может служить запись о пользователе, в которой есть строковые значения почты, даты создания, целочисленный counter, массив (последовательность) строковых ролей: 
```
{
  "Email": "james@example.com",
  "Active": true,
  "CreatedDate": "2013-01-20T00:00:00Z",
  "Roles": [
    "User",
    "Admin"
  ],
  "Counter": 67
}
```

Работа с подобными стандартизованными форматами данных, как правило, многократно реализована в разных вариантах. Надо только найти подходящую библиотеку. Я выбрал библиотеку Newtonsoft. В частности, библиотека имеет свою систему NuGet пакетов, что делает ее удобной для применения. Библиотека, в той части которая нам интересна, довольно простая и не требует осваивать новые модели. Она осуществляет преобразование специальных структур языка C# в JSON-объекты и наоборот. 

В программе, приложенной к данному заданию, можно легко разораться в том, что делается. Сначала (стр. 17-27) устанавливается значение структуры, тип которой определен строчками 128-134. Это все в соотвествии с правилами C#. Далее, преобразования SerializeObject, DeserializeObject выполняют сериализацию в формат JSON заданной структуры и десериализацию текста в объект заданного класса.    
```
    string json = JsonConvert.SerializeObject(account, Formatting.Indented);
    Console.WriteLine(json);

    var ob = JsonConvert.DeserializeObject<Account>(json);
```
Вот и все! 

Для закрепления пройденного материала и для измерения времен обработки, делается следующий фрагмент программы, выстроенной в логике первой таблицы теста "фототека". Сначала делается пробная база данных для того, чтобы показать вид формируемой базы данных. После этого, генерируется база данных содержащая 1 млн. элементов. Как видно из примера, база данных представляет собой список записей персон. Этот список формируется LINQ-выражением и представляет собой массив структур типа Person. Этот массив сериализуется библиотечной процедурой. И полученный текст записывается в текстовый файл. Не слишком экономная процедура. Но после чтения теста из файла и его десериализации, мы получаем массив в оперативной памяти с выдающимися характеристиками по доступу к данным. 

Последнюю часть программы можно не читать. Делалась попытка вместо JSON использовать бинарный формат BSON. Попытка была реализована, но получилось, что на использованной задаче (множество персон), выигрыша бинарный формат не несет ни по одному из измеренных свойств (запись в файл, чтение из файла). 

### Task09_TCP - основы сетевого программирования
Большие данные - это еще и распределенные данные. Для обработки распределенных данных приходится создавать эффективные сетевые решения. 

TCP - Tranfer Communication Protocol, это протокол, который, наряду с UDP, лежит в основе почти всех сетевых решений. Есть средства более низкого уровня, но они практически не используются в прикладном программировании, UDP мы также не будем "трогать", он подходит к другим задачам. Займемся TCP. Как вообще происходит сетевое взаимодействие в рамках сложившейся архитектуры IP (Internet Protocol)? Есть две программы, в общем случае располагающиеся на разных машинах. Взаимодействие несимметрично, поэтому одну программу будем называть "сервер" или "сервис", а другую - клиент. Сервер на какой-то порт "вешает" ожидающий процесс, который называется Listener - "слушатель". У этого слушателя есть ip-адрес машины сервера и номер порта. Клиент обращается по этой паре адрес-порт с запросом на соединение. В нормальном случае соединение происходит и программы могут обмениваться пакетами информации. Как они устроены, нам не важно, потому что TCP порождает специально организованную среду общения. Эта среда общения выглядит как двунаправленный поток (Stream) между программами. Одна программа может посылать поcледовательность байтов, другая - принимать. И совершенно также передается информация в обратном направлении. TCP гарантирует то, что байты, посланные позже и будут приняты позже, в том же порядке, что и посылались. Принципиальным является то, что байты, которые посылаются должны быть приняты противоположным агентом (программой). Возможно два вида проблемных ситуаций. В первой - другой агент перестал принимать посылаемые ему байты и сам делает попытку посылать - это приведет к появлению исключительной ситуации и разрушению связи. Вторая проблемная ситуация заключается в том, что посылающий агент закончил посылку своей информации и начал прием, а принимающий агент все еще ждет прихода информации. Эта ситуация разрешается по времени через timeout. И снова - exception и разрушение связи. 

Перейдем к программе реализации. Все решения мы будем выполнять в рамках одной программы, задавая разные параметры (аргументы) запуска. Начнем с сервера. Для программы нам понадобятся некоторые пространства имен, также добавим определения для ip-адреса компьютера host и номера порта:  
```
using System;
using System.IO;
using System.Net;
using System.Net.Sockets;
using System.Threading.Tasks;
namespace Task09_TCP
{
    class Program
    {
        private static IPAddress host = IPAddress.Parse("127.0.0.1");
        private static int port = 5000;
        public static void Main(string[] args)
        {
            Console.WriteLine("Start TCP Server");
            //Server();
        }
    }
}
```
Теперь можно начинать. Поскольку программа Main будет основой для других программ, выделим собствено сервер в отдельный метод, который вызовем из Main.
```
    static void Server()
    {
        TcpListener listener = new TcpListener(host, port);
        listener.Start();
        var client = listener.AcceptTcpClient();
        Console.WriteLine("client accepted");
        var stream = client.GetStream();
        byte[] buff = new byte[1000];

        // Принимаем
        int nbytes = stream.Read(buff, 0, buff.Length);
        string received = System.Text.Encoding.UTF8.GetString(buff, 0, nbytes);
        Console.WriteLine(received);
        if (received == "exit") break;

        // Посылаем
        string message = "OK";
        byte[] arr = System.Text.Encoding.ASCII.GetBytes(message);
        stream.Write(arr, 0, arr.Length);

        stream.Close();
        client.Close();
        listener.Stop();
    }
```
Сервер выглядит довольно простым. Действительно, сначала мы формируем слушателя и запускаем его. Далее, предполагается ожидание появления клиента установка и подтверждение AcceptTcpClient связи. При этом формируется как-бы клиент, точнее, его местный представитель. 

Логику работы сервера мы устраиваем наиболее традиционным образом. Сервер ждет клиента организует с ним связь и ждет сообщения от клиента. Сообщение - некоторый конечный поток байтов. Сервер его принимает, распечатывает и возвращает свой поток байтов, для начала, мы будем возвращать пару байтов: "OK". После этого, сервер закончит свою деятельность, закрыв все, что можно закрыть. 

А как его проверить? Воспользуемся браузером в качестве клиента. В принципе, любой браузер посылает указанному серверу на указанный порт какое-то сообщение и ждет сообщения от сервера. Берем браузер, напр. Microsoft Edge, набираем адресную строку http://localhost:5000 и смотрим что получается. Скорее всего, информационный пакет от браузера серверу дойдет и напечатается на серверной консоли. Те, кто разбирается в протоколе HTTP, поймут эти строки. А вот обратное сообщение, от сервера клиенту (мы посылаем OK), может и не дойти. Используйте Edge - есть шанс получить "OK". А сервер, как мы его запрограммировали, остановится и связь разорвется.

В таком подходе есть недостатки. Наш сервер приходится все время запускать ради отработки одного обращения. Логично организовать цикл. Слушателя организовывать и стартовать не надо, а вот подтверждать подключение надо, соответственно, программу сервера можно модифицировать следующим образом:
```
    static void Server()
    {
        TcpListener listener = new TcpListener(host, port);
        listener.Start();
        while(true)
        {
            var client = listener.AcceptTcpClient();
            Console.WriteLine("client accepted");
            var stream = client.GetStream();
            byte[] buff = new byte[1000];

            // Принимаем
            int nbytes = stream.Read(buff, 0, buff.Length);
            string received = System.Text.Encoding.UTF8.GetString(buff, 0, nbytes);
            Console.WriteLine(received);
            if (received == "exit") break;

            // Посылаем
            string message = "OK";
            byte[] arr = System.Text.Encoding.ASCII.GetBytes(message);
            stream.Write(arr, 0, arr.Length);

            stream.Close();
            client.Close();
        }
        listener.Stop();
    }
```  
Сервер начинает работать. С браузером Microsoft Edge получается неплохо. Просле запуска сервера, я могу произвольное число раз посылать пустой запрос, в ответ, приходит "OK". Чтобы лучше видеть смену ответного сообщения, можно добавить в него текущее время, как-нибудь так:
```
    string message = "OK " + DateTime.Now.ToString();
```  
Мы видем, что первый запрос отрабатывается, а вот следующий - нет. Более того, браузер зависает в каком-то ожидании. В чем дело? Дело в том, что протокол HTTP, который предполагается между браузером и сервером, хоть и построен на TCP, но предполагает вполне определенный вид информационных посылок туда-сюда. В частности, сервер должен ответить особым образом для того, чтобы браузер воспринял этот ответ. Возможный вариант ответа (число 6 - количество байтов в основном теле информационного пакета, в данном случае - в послании Hello!):
```
HTTP/1.0 200 OK
Content-Type: text/plain
Content-Length: 6

Hello!
``` 
Изменив возвращаемой от сервера сообщение на указанное мы, скорее всего, добъемся циклической работы программы. 

Двинемся дальше. Теперь сделаем клиента. И не HTTP-клиента, а простого TCP-клиента. Вернмся к коротким обменным сообщениям фиксированного размера. Пусть клиент посылает строку "request", состоящую из 7 байтов, а сервер отвечает строкой "OK", состоящей из 2 байтов. 

Чтобы не заводить для клиента новый проект, будем в программе анализировать первый аргумент и если он - строка "client", то запускаться должен клиент, в противном случае - сервер, который у нас уже имеется. 

Программа Client будет устроена также максимально просто:
```    
    static void Client()
    {
        TcpClient client = new TcpClient();
        client.Connect(host, port);
        Console.WriteLine("client connected");

        var stream = client.GetStream();
        byte[] buff = new byte[1000];

        // Посылаем
        byte[] arr = System.Text.Encoding.ASCII.GetBytes("request");
        stream.Write(arr, 0, arr.Length);

        // Принимаем
        int nbytes = stream.Read(buff, 0, 3);// buff.Length);
        string received = System.Text.Encoding.UTF8.GetString(buff, 0, nbytes);
        Console.WriteLine(received);

        client.Close();
    }
```
Тут все очевидно. Создается TcpClient, он подключается к удаленному порту через указанные (host, port), из него берется Stream, а далее - как обычно - посылаем и принимаем информационные сообщения. На двух консолях опробуем получившуюся конструкцию. Но одной запустим программу без аргументов, на другой - с аргументом client. В режиме обычного пуска это выглядит как:
```
dotnet run client
```
Все работает. Можно несколько раз запустить клиента, каждый раз клиент коннектится, посылает сообщение, принимает сообщение и "отваливается". Только запуск происходит долго. Существенно быстрее запуск произойдет если запускать напрямую .dll программы, напр.:
```
dotnet bin\Debug\netcoreapp2.0\Task09_TCP.dll client
```
Естественно, захочется измерить производительность такой клиент-серверной системы, для этого, достаточно написать в клиенте цикл. Кстати, если Вы произвели в программе изменения, необходимо "загасить", напр. с помощью <cntrl>C еще работающие программы, иначе компиляция не выполнится и новый вариант программы запустить не получится.

Испытание показывает, что такая пара клиент-сервер показывает 800 мс. на 1000 взаимодействий. Это на моем ноутбуке. А на стационарном рабочем компьютере, это около 100 мс. на 1000 взаимодействий.

TCP позволяет осуществлять диалог и без разрыва соединения. Я уже писал, что после создания соединения, на каждом конце образуется стрим, которых предназначен и для чтения и для записи, важно только соблюдать порядок, что когда один пишет, другой читает. Наш протокол (клиент посылает "request", сервер возвращает "OK"), может быть продолжен на произвольное число циклов. Для этого, в сервере добавим внутренний цикл, прерываемый по приказу "exit". В клиенте можно просто цикл перенести глубже, к фазам посылка-прием. Чтобы не загромождать получающуюся программу, новые варианты сервера и клиента назовем MultiServer, MultiClient.

Испытание и замер производительности показывают, что все работает и показывает около 100 мс. на 1000 обменов (1050 мс. на 10 тыс. обменов), это на ноутбуке. На рабочем компьюетер получилась скорость около 20 мс. на 1000 обменов (185 мс. на 10 тыс.).

Но все предыдущие "сетевые" измерения скорости, были сделаны на одном компьютере, т.е. без реальной передачи данных по сети... 

### Task10_StreamStorage
База данных, хранилище, репозиторий, это практически всегда множество файлов. Точнее - множество потоков (Streams). Поток (Stream) - это последовательность байтов фиксированного или растущего размера. Обычное место "обитания" потоков - файловая система. Файловая система, реализуемая той или иной операционной системой очень неплохо оптимизирована на эффективную работу с файлами. С этим мы уже сталкивались, когда анализировали производительность процессов доступа к информации, размещенной в файловой системе. Было показано, что в современных ОС существует и используется системный кеш, существенно ускоряющий произвольную выборку данных. И запись также. 

Однако, особенно для данных большого объема, бывает целесообразным использовать специализированное хранилище, это может быть специально выполненная и специально организованная файловая структура. Простыми решениями является упаковщики tar, zip и др. Но у них есть недостатки, связанные с эффективностью доступа к отдельным потокам и, особенно, - в поддержании растущих потоков. Для чего это нужно? Есть несколько причин работы с хранилищем потоков. Во-первых, часто удобно, когда вся конструкция какой-то базы данных выглядело бы как один файл. Во-вторых, потоков в конкретной ситуации может быть много или очень много, например - миллионы. Если пользоваться универсальной файловой системой, то база данных или хранилище могут стать неудобными для выполнения таких естественных действий, как копирование. В-третьих, к специализированному хранилищу потоков, можно "приделать" свою кешевую систему, учитывающую особенности его применения. Или систему резервного копирования. 

Собственно, некоторые из приведенных задач будут проиллюстрированы в данном задании.










### Task11_UniversalNode
Рассмотрим следующую постановку. Пусть у нас есть класс, по которому можно делать объект(ы). В принципе, нормальный интерфейс к такому описанию - генератор new и методы доступа к объекту. Для начала, этот класс мы погружаем в библиотеку и можем "снаружи" пользоваться методами как обычно. Возможно будет разумным сделать объект на основе этого класса. Ну типа статического объекта. Появляется узел. В нем есть "главный" объект, к нему есть методы. Вопрос генерации нового значения можно пока отложить.

Теперь добавим к нему сервис. Все очень просто, все типы значений, появляющихся в параметрах и результатах, должны быть сериализуемыми. Тогда совсем просто породить сетевой интерфейс, напр. средствами TCP, который будет полностью соответствовать интерфейсу опорного класса. Средствами бинарной сериализации я уже пользовался, что-то получалось. 

Следующее расширение - консольный интерфейс. Опять же средствами консоли можно сделать реализацию все тех же методов опорного объекта. Для этого, достаточно в тектовом пространстве указывать имя метода, и перечислять фактические параметры в виде текстовой развертки соответствующих значений. Получился Shell, а если ключевые методы будут формировать или потреблять потоки IEnumerable, то уже мы близко к LINQ. А если в параметрах методов можно задавать делегаты, то потребуется нотация для построения LINQ-формул. В этом месте существует "развилка": или можно начать вызывать компилятор, или надо писать свой синтаксический анализ или можно поступить по новаторски. По новаторски, это породить формульный язык в виде поляр-структур. Не факт, что будет удобно на этом языке задавать фрагменты обработки. Зато все будет идейно и, возможно, эффективно. 





## Приложение А. Элементы теории поляровской структуризации

Структурное значение (structured value, поляровское структурное значение, p-value) - древовидное построение (значение), интерпретируемое в соответствии с заданным типом.

Тип (Type, поляровский тип, p-type) - древовидное построение, задающее интерпретацию для структурных значений. Может быть выражено как структурное значение. Тип может быть примитивным (атомарным) или конструируемым (составным).

Объект базы данных - сформированное в некотором рабочем поле или в хранилище структурное значение.

Объектное представление структурного значения - внутренняя конструкция в ОЗУ, вместе с типовым значением, задающая структурное значение. Объектное представление реализуется средствами какой-то (напр. .NET) системы программирования, доступно через какой-то язык программирования и служит связующим средством между программными объектами и объектами базы данных по следующей схеме: объект базы данных полностью или частично, отдельными полями, может быть реализован значениями объектного представления. Соответственно, возможно "чтение" объектов базы данных или их полей в объектное представление. И наоборот, возможна "запись" значений объектного представления в объекты базы данных или их поля.

Текстовая сериализация (текстовое представление) структурного значения - точные правила представления любого структурного значения последовательностью символов. Текстовое представление, совместно с явно или неявно определенным типом этого представления, есть структурное значение

Бинарная (байтовая) сериализация структурного значения - правила представления любого структурного значения в виде потока байтов. Бинарная сериализация осуществляется только в контексте типа сериализуемого или десериализуемого структурного значения.

Примитивные типы:

boolean, character, integer, longinteger, real - логическое, символьное, целое (32 разряда), длинное целое (64 разряда), число с плавающей точкой (64 разряда),
@byte - байт - 8-разрядный код,
none - множество значений, не содержащее ни одного элемента,
sstring - обычные строки объектно-ориентированного программирования (C#).

Конструируемые типы:

Запись (record) - фиксированный набор типизированных полей. Есть поля 0, 1, ... n-1, каждое из которых представляет значение заданного в определении записи типа. Количество полей - фиксировано, типы полей - фиксированы. Иногда полям сопоставляют имена, являющиеся идентификаторами.

Последовательность (sequence) - упорядоченный набор, состоящий из неопределенного (ноль или более), но конкретного, числа однотипных элементов.

Объединение (union) - значение, состоящее из тега и подзначения. Тег (динамически) определяет вариант типа для подзначения. Теги нумеруются, начиная с 0.

Объектное представление. Любое структурное значение может быть представлено в виде объекта по следующей схеме: примитивные типы реализуются значениями соответствующих системных типов - логического, целого, длинного целого, с плавающей точкой (double), символьного (char), байта, строки. Значения типа none представляются значениями null. Конструируемые типы представляются массивами object[], элементами которых будут подзначения структур. Запись - массив объектных значений своих элементов, последовательность - массив объектных значений элементов. Значение объединенного типа представляет собой массив из двух элементов. Первый элемент - целое значение тега. Второй элемент - объектное представление подзначения соответствующего тегу типа.

Примеры. Объектные значения (object)22, (object)"demo string", (object)true могут быть проинтерпретированы только как целое, строка и логическое. Объект new object[] {2, 33} может быт проинтерпретирован в зависимости от типа или как запись двух целых или как последовательность целых или как объединение с вариантом 2 и подзначением 33. Для последовательности записей, состоящих из строкового и целого, значением в объектном представлении может быть: new object { new object[] {"str1", 111}, new object[] {"str2", 222} }

Текстовая сериализация. Атомарные значения изображаются в виде текста: типа none - пустой строкой, типа boolean, character, integer, longinteger, real - традиционно, как напр. в C#. @byte изображается 16-ричным кодом, строка - как обычно (в C#). Запись изображается перечислением через запятую значений всех полей записи, начиная с нулевого и далее по порядку. Возможно использования имен полей (как в Паскале). Последовательность, изображается перечислением элементов через запятую, все перечисление (нуль или более элементов) помещается в квадратные скобки. Объединение изображается тегом - числом в диапазоне 0-255, следующим за ним символом ^ и далее идет изображение значения того типа, который динамически задан тегом. Значение может быть помещено в круглые скобки, это нужно для определенности разбора. По структурному значению и его типу однозначно, с точности до несущественных синтаксических элементов (пробелы, перевод строки, tab) и "лишних" скобок, определяется текстовая развертка. По корректной текстовой развертке и типу, однозначно определяется структурное значение.

Примеры. Для структурного значения типа последовательности записей строкового и целого полей, заданного в объектном виде как new object { new object[] {"str1", 111}, new object[] {"str2", 222} }, текстовая сериализация будет: [ {"str1", 111}, {"str2", 222} ].

Бинарная сериализация. Бинарная сериализация выполняется следующим образом: атомарные значения отображаются на последовательность байтов так, как определяется системой программирования C# через метод System.IO.BinaryWriter и двойственный ему System.IO.BinaryReader. Байт отображается в байт, целое в 4 байта, длинное целое - в 8 байтов, число с плавающей точкой в 8 байтов, значение типа none - в 0 байтов. Строка отображается сложнее. Сначала идет целое (4 байта) число, равное количеству символов в строке. Далее, если количество символов больше 0, то идут байты, сформированные по правилам UTF-8: byte[] info = new UTF8Encoding(true).GetBytes(str); Запись сериализуется поставленными "встык" сериализованными значениями полей. Последовательность вначале имеет 64-разрядное (8 байтов) целое, фиксирующее количество элементов последовательности (0+), а за ним подряд идут соответствующее количество значений элементов последовательности. Объединение реализуется постановкой 1 байта, который фиксирует код тега (0-255), за этим байтов непосредственно следует подзначение соответствующего тегу типа.

Язык определения типов. Поляровские типы задаются некоторыми формулами, задающими типовые (ударение на первый слог) значения. В принципе, эти значения можно рассматривать как обычные структурные значения, но эта ветвь в текущем варианте Поляра не реализована. К тому же, работать с заданием типа в таком представлении неудобно. Поэтому, для наглядности, в иллюстрациях используется специальный язык определения типов. Синтаксис языка в БНФ:
```
типовое_определение: имя_типа = типовая_формула ;
типовая_формула: none
     | bool | byte | char | int | long | float
     | { (имя поля : )? типовая_формула,.. }
     | [ типовая_формула ]
     | имя_тега ^ типовая формула
```
Синтаксические особенности и добавления. В формуле определения записи возможно использование имен полей. В случае, когда определяется последовательность записей, фигурные скобки внутри прямоугольных можно опустить.

## Конец Приложения А.